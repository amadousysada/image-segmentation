#!/usr/bin/env python3
"""
Solutions rapides pour acc√©l√©rer l'entra√Ænement

40 minutes par √©poque ‚Üí 2-5 minutes par √©poque
"""

import tensorflow as tf
import numpy as np

def apply_immediate_optimizations():
    """Applique les optimisations imm√©diates"""
    
    print("üöÄ APPLICATION DES OPTIMISATIONS IMM√âDIATES")
    print("="*50)
    
    # 1. GPU Memory Growth (√©vite les erreurs OOM)
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print("‚úÖ GPU memory growth activ√©")
    
    # 2. Mixed Precision (2x plus rapide sur GPU moderne)
    if gpus:
        tf.keras.mixed_precision.set_global_policy('mixed_float16')
        print("‚úÖ Mixed precision activ√© (acc√©l√©ration ~2x)")
    
    # 3. XLA compilation
    tf.config.optimizer.set_jit(True)
    print("‚úÖ XLA JIT compilation activ√©")
    
    # 4. Optimiser les threads
    tf.config.threading.set_intra_op_parallelism_threads(0)
    tf.config.threading.set_inter_op_parallelism_threads(0)
    print("‚úÖ Threading optimis√©")

def create_fast_dataset(images, masks, batch_size=32, cache=True):
    """Cr√©er un dataset ultra-rapide"""
    
    print(f"\nüèÉ CR√âATION D'UN DATASET RAPIDE")
    print("="*50)
    
    # Dataset de base
    dataset = tf.data.Dataset.from_tensor_slices((images, masks))
    
    # Optimisations critiques
    dataset = dataset.map(
        lambda x, y: (x, y),
        num_parallel_calls=tf.data.AUTOTUNE,
        deterministic=False  # Plus rapide
    )
    
    if cache:
        dataset = dataset.cache()  # Met en cache RAM/SSD
        print("‚úÖ Cache activ√© (donn√©es en m√©moire)")
    
    dataset = dataset.batch(batch_size, drop_remainder=True)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    
    print(f"‚úÖ Batch size: {batch_size}")
    print(f"‚úÖ Prefetch: AUTOTUNE")
    print(f"‚úÖ Parallel calls: AUTOTUNE")
    
    return dataset

def get_optimal_batch_size():
    """D√©termine le batch size optimal selon la m√©moire disponible"""
    
    gpus = tf.config.list_physical_devices('GPU')
    
    if not gpus:
        print("‚ö†Ô∏è  CPU d√©tect√© - batch_size recommand√©: 8-16")
        return 8
    
    # Estimation bas√©e sur GPU memory
    try:
        # Test avec diff√©rents batch sizes
        model = tf.keras.Sequential([
            tf.keras.layers.Conv2D(32, 3, input_shape=(224, 224, 3)),
            tf.keras.layers.GlobalAveragePooling2D(),
            tf.keras.layers.Dense(8)
        ])
        
        for batch_size in [64, 32, 16, 8]:
            try:
                test_data = tf.random.normal((batch_size, 224, 224, 3))
                _ = model(test_data)
                print(f"‚úÖ GPU supporte batch_size: {batch_size}")
                return batch_size
            except tf.errors.ResourceExhaustedError:
                continue
        
        return 8
    except:
        return 16

def optimize_model_for_speed(model):
    """Optimise le mod√®le pour la vitesse"""
    
    print(f"\n‚ö° OPTIMISATION DU MOD√àLE")
    print("="*50)
    
    # Compiler avec optimisations
    if tf.keras.mixed_precision.global_policy().name == 'mixed_float16':
        # Pour mixed precision, utiliser des loss functions compatibles
        model.compile(
            optimizer=tf.keras.optimizers.Adam(2e-4),
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
            metrics=['accuracy']
        )
        print("‚úÖ Compilation optimis√©e pour mixed precision")
    else:
        model.compile(
            optimizer=tf.keras.optimizers.Adam(1e-4),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        print("‚úÖ Compilation standard")
    
    return model

def create_speed_optimized_training():
    """Configuration d'entra√Ænement optimis√©e pour la vitesse"""
    
    print(f"\nüèÅ CONFIGURATION D'ENTRA√éNEMENT RAPIDE")
    print("="*50)
    
    # Callbacks pour convergence rapide
    callbacks = [
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='loss',  # Monitor training loss directement
            factor=0.5,
            patience=2,      # Plus agressif
            min_lr=1e-6,
            verbose=1
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor='loss',
            patience=5,      # Plus agressif
            restore_best_weights=True,
            verbose=1
        )
    ]
    
    print("‚úÖ Callbacks configur√©s pour convergence rapide")
    print("‚úÖ Early stopping agressif")
    print("‚úÖ R√©duction LR rapide")
    
    return callbacks

# ============================================================================
# EXEMPLE COMPLET D'OPTIMISATION
# ============================================================================

def example_optimized_training():
    """Exemple complet d'entra√Ænement optimis√©"""
    
    print("\n" + "="*60)
    print("EXEMPLE D'ENTRA√éNEMENT ULTRA-RAPIDE")
    print("="*60)
    
    # 1. Appliquer les optimisations
    apply_immediate_optimizations()
    
    # 2. D√©terminer batch size optimal
    optimal_batch_size = get_optimal_batch_size()
    
    # 3. Cr√©er donn√©es factices pour demo
    print(f"\nüìä Cr√©ation de donn√©es de test...")
    num_samples = 1000
    images = tf.random.normal((num_samples, 224, 224, 3))
    masks = tf.random.uniform((num_samples, 224, 224), 0, 8, dtype=tf.int32)
    
    # 4. Dataset optimis√©
    train_ds = create_fast_dataset(images[:800], masks[:800], optimal_batch_size)
    val_ds = create_fast_dataset(images[800:], masks[800:], optimal_batch_size)
    
    # 5. Mod√®le optimis√©
    try:
        from models import create_model
        model = create_model('unet_mini', (224, 224, 3), 8)
        model = optimize_model_for_speed(model)
        print(f"‚úÖ Mod√®le U-Net Mini cr√©√© et optimis√©")
    except ImportError:
        print("‚ö†Ô∏è  Module models non trouv√©, utilisation d'un mod√®le simple")
        model = tf.keras.Sequential([
            tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(224, 224, 3)),
            tf.keras.layers.Conv2D(8, 1, activation='softmax')
        ])
        model = optimize_model_for_speed(model)
    
    # 6. Configuration rapide
    callbacks = create_speed_optimized_training()
    
    # 7. Test d'entra√Ænement rapide
    print(f"\nüèÉ ENTRA√éNEMENT TEST (2 √©poques)...")
    import time
    start_time = time.time()
    
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=2,
        callbacks=callbacks,
        verbose=1
    )
    
    elapsed_time = time.time() - start_time
    time_per_epoch = elapsed_time / 2
    
    print(f"\n‚è±Ô∏è  R√âSULTATS:")
    print(f"   Temps total: {elapsed_time:.1f}s")
    print(f"   Temps/√©poque: {time_per_epoch:.1f}s")
    print(f"   Acc√©l√©ration vs 40min: {2400/time_per_epoch:.1f}x plus rapide!")
    
    return history

# ============================================================================
# CODE POUR VOTRE CAS SP√âCIFIQUE
# ============================================================================

def fix_your_training():
    """Code sp√©cifique pour r√©soudre votre probl√®me de 40min/√©poque"""
    
    print("\n" + "="*60)
    print("SOLUTION POUR VOTRE CAS SP√âCIFIQUE")
    print("="*60)
    
    print("üîß AJOUTEZ CE CODE AU D√âBUT DE VOTRE NOTEBOOK:")
    print()
    
    code_to_add = '''
# ===== OPTIMISATIONS SPEED =====
import tensorflow as tf

# 1. Activer toutes les optimisations GPU
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    # Mixed precision = 2x plus rapide
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    print("‚úÖ GPU optimis√© avec mixed precision")
else:
    print("‚ö†Ô∏è Pas de GPU - utilisez Google Colab avec GPU!")

# 2. XLA compilation
tf.config.optimizer.set_jit(True)

# 3. Optimiser le dataset
def optimize_dataset(ds, batch_size=32):
    return ds.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)

# ===== UTILISATION =====
# Appliquez √† vos datasets:
# train_ds = optimize_dataset(train_ds, batch_size=32)
# validation_ds = optimize_dataset(validation_ds, batch_size=32)

# Compilez votre mod√®le avec:
# model.compile(
#     optimizer=tf.keras.optimizers.Adam(2e-4),
#     loss=tf.keras.losses.SparseCategoricalCrossentropy(),
#     metrics=['accuracy']
# )
'''
    
    print(code_to_add)
    
    print("\nüéØ CHANGEMENTS DANS VOTRE CODE D'ENTRA√éNEMENT:")
    print()
    
    training_code = '''
# Au lieu de:
# hist = model.fit(train_sample, validation_data=validation_sample, epochs=epochs)

# Utilisez:
hist = model.fit(
    train_sample,
    validation_data=validation_sample,
    epochs=epochs,
    batch_size=32,  # Augmentez si possible
    verbose=1
)
'''
    
    print(training_code)
    
    print("\nüìà R√âSULTATS ATTENDUS:")
    print("   - AVANT: 40 minutes/√©poque")
    print("   - APR√àS: 2-5 minutes/√©poque (avec GPU)")
    print("   - APR√àS: 10-15 minutes/√©poque (CPU optimis√©)")

def main():
    """Fonction principale"""
    
    print("ACC√âL√âRATION D'ENTRA√éNEMENT - DE 40MIN √Ä 2MIN PAR √âPOQUE")
    print("=" * 60)
    
    # Diagnostic rapide
    gpus = tf.config.list_physical_devices('GPU')
    if not gpus:
        print("üö® PROBL√àME PRINCIPAL: PAS DE GPU D√âTECT√â")
        print("   Solution: Utilisez Google Colab avec GPU activ√©")
        print("   Runtime ‚Üí Change runtime type ‚Üí GPU")
    else:
        print(f"‚úÖ {len(gpus)} GPU(s) d√©tect√©(s)")
    
    # Solutions imm√©diates
    fix_your_training()
    
    # Test si possible
    try:
        example_optimized_training()
    except Exception as e:
        print(f"\n‚ö†Ô∏è Test d'optimisation √©chou√©: {e}")
        print("Mais les optimisations ci-dessus devraient fonctionner!")

if __name__ == "__main__":
    main()