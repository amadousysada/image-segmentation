#!/usr/bin/env python3
"""
Script d'optimisation des performances d'entra√Ænement

Diagnostic et solutions pour acc√©l√©rer l'entra√Ænement des mod√®les de segmentation.
"""

import time
import tensorflow as tf
import os
import psutil
import sys

def diagnose_system():
    """Diagnostic du syst√®me pour identifier les goulots d'√©tranglement"""
    
    print("="*60)
    print("DIAGNOSTIC DU SYST√àME")
    print("="*60)
    
    # GPU
    gpu_devices = tf.config.list_physical_devices('GPU')
    print(f"üñ•Ô∏è  GPU disponibles: {len(gpu_devices)}")
    
    if gpu_devices:
        for i, gpu in enumerate(gpu_devices):
            print(f"   GPU {i}: {gpu.name}")
        
        # V√©rifier si GPU est utilis√©
        print(f"üîß GPU configur√©: {tf.test.is_gpu_available()}")
        print(f"üèÉ GPU actif: {tf.test.is_built_with_cuda()}")
    else:
        print("‚ö†Ô∏è  AUCUN GPU D√âTECT√â - Utilisation CPU uniquement")
        print("üí° Solution: Utiliser Google Colab avec GPU ou installer CUDA")
    
    # M√©moire
    memory = psutil.virtual_memory()
    print(f"\nüíæ RAM totale: {memory.total / (1024**3):.1f} GB")
    print(f"üíæ RAM disponible: {memory.available / (1024**3):.1f} GB")
    print(f"üíæ RAM utilis√©e: {memory.percent:.1f}%")
    
    # CPU
    print(f"\nüî≤ CPU cores: {psutil.cpu_count()}")
    print(f"üî≤ CPU usage: {psutil.cpu_percent():.1f}%")
    
    # TensorFlow config
    print(f"\n‚öôÔ∏è  TensorFlow version: {tf.__version__}")
    print(f"‚öôÔ∏è  Mixed precision: {tf.keras.mixed_precision.global_policy().name}")

def optimize_tensorflow():
    """Optimiser la configuration TensorFlow"""
    
    print("\n" + "="*60)
    print("OPTIMISATION TENSORFLOW")
    print("="*60)
    
    # 1. Configuration GPU
    gpu_devices = tf.config.list_physical_devices('GPU')
    if gpu_devices:
        try:
            # Permettre la croissance m√©moire GPU
            for gpu in gpu_devices:
                tf.config.experimental.set_memory_growth(gpu, True)
            print("‚úÖ Croissance m√©moire GPU activ√©e")
        except:
            print("‚ö†Ô∏è  Impossible de configurer la croissance m√©moire GPU")
    
    # 2. Mixed Precision (acc√©l√©ration sur GPU moderne)
    if gpu_devices:
        try:
            tf.keras.mixed_precision.set_global_policy('mixed_float16')
            print("‚úÖ Mixed precision activ√©e (float16)")
        except:
            print("‚ö†Ô∏è  Mixed precision non support√©e")
    
    # 3. XLA (compilation optimis√©e)
    try:
        tf.config.optimizer.set_jit(True)
        print("‚úÖ XLA JIT compilation activ√©e")
    except:
        print("‚ö†Ô∏è  XLA non disponible")
    
    # 4. Parallel processing
    tf.config.threading.set_intra_op_parallelism_threads(0)  # Use all available
    tf.config.threading.set_inter_op_parallelism_threads(0)  # Use all available
    print("‚úÖ Parall√©lisme optimis√©")

def create_optimized_dataset(image_paths, mask_paths, image_size=224, batch_size=16):
    """Cr√©er un dataset optimis√© pour les performances"""
    
    print("\n" + "="*60)
    print("OPTIMISATION DU DATASET")
    print("="*60)
    
    def parse_function(image_path, mask_path):
        """Fonction de parsing optimis√©e"""
        # Lecture des fichiers
        image = tf.io.read_file(image_path)
        mask = tf.io.read_file(mask_path)
        
        # D√©codage
        image = tf.image.decode_png(image, channels=3)
        mask = tf.image.decode_png(mask, channels=1)
        
        # Redimensionnement
        image = tf.image.resize(image, [image_size, image_size])
        mask = tf.image.resize(mask, [image_size, image_size], method='nearest')
        
        # Normalisation
        image = tf.cast(image, tf.float32) / 255.0
        mask = tf.cast(mask, tf.int32)
        mask = tf.squeeze(mask, axis=-1)
        
        return image, mask
    
    # Cr√©er le dataset
    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))
    
    # Optimisations
    dataset = dataset.map(
        parse_function, 
        num_parallel_calls=tf.data.AUTOTUNE,
        deterministic=False  # Plus rapide mais non d√©terministe
    )
    
    dataset = dataset.cache()  # Met en cache en m√©moire
    dataset = dataset.shuffle(1000)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    
    print(f"‚úÖ Dataset optimis√© cr√©√©:")
    print(f"   - Batch size: {batch_size}")
    print(f"   - Parall√©lisme: AUTOTUNE")
    print(f"   - Cache: Activ√©")
    print(f"   - Prefetch: AUTOTUNE")
    
    return dataset

def benchmark_model_speed(model, input_shape=(224, 224, 3), batch_sizes=[4, 8, 16, 32]):
    """Benchmark la vitesse du mod√®le avec diff√©rents batch sizes"""
    
    print("\n" + "="*60)
    print("BENCHMARK DE VITESSE")
    print("="*60)
    
    results = {}
    
    for batch_size in batch_sizes:
        print(f"\nüß™ Test batch_size = {batch_size}")
        
        # Cr√©er donn√©es de test
        test_data = tf.random.normal((batch_size, *input_shape))
        
        # Warmup
        for _ in range(3):
            _ = model(test_data, training=False)
        
        # Benchmark
        times = []
        for _ in range(10):
            start = time.time()
            _ = model(test_data, training=False)
            times.append(time.time() - start)
        
        avg_time = sum(times) / len(times)
        samples_per_sec = batch_size / avg_time
        
        results[batch_size] = {
            'time_per_batch': avg_time,
            'samples_per_sec': samples_per_sec
        }
        
        print(f"   Temps/batch: {avg_time:.3f}s")
        print(f"   √âchantillons/sec: {samples_per_sec:.1f}")
    
    # Recommandation
    best_batch = max(results.keys(), key=lambda x: results[x]['samples_per_sec'])
    print(f"\nüí° RECOMMANDATION: Batch size optimal = {best_batch}")
    print(f"   Performance: {results[best_batch]['samples_per_sec']:.1f} √©chantillons/sec")
    
    return results

def create_fast_training_setup():
    """Cr√©er une configuration d'entra√Ænement rapide"""
    
    print("\n" + "="*60)
    print("CONFIGURATION D'ENTRA√éNEMENT RAPIDE")
    print("="*60)
    
    # Callbacks optimis√©s
    callbacks = [
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.7,
            patience=3,  # Plus agressif
            min_lr=1e-7,
            verbose=1
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=8,  # Plus agressif
            restore_best_weights=True,
            verbose=1
        )
    ]
    
    # Optimiseur rapide
    optimizer = tf.keras.optimizers.Adam(
        learning_rate=2e-4,  # Plus √©lev√© pour convergence rapide
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-7
    )
    
    print("‚úÖ Configuration cr√©√©e:")
    print("   - Callbacks agressifs pour convergence rapide")
    print("   - Learning rate optimis√©")
    print("   - Adam optimizer configur√©")
    
    return callbacks, optimizer

def estimate_training_time(num_samples, batch_size, epochs, samples_per_sec):
    """Estimer le temps d'entra√Ænement"""
    
    batches_per_epoch = num_samples // batch_size
    total_batches = batches_per_epoch * epochs
    estimated_time = total_batches / (samples_per_sec / batch_size)
    
    print(f"\n‚è±Ô∏è  ESTIMATION DE TEMPS:")
    print(f"   √âchantillons: {num_samples}")
    print(f"   Batch size: {batch_size}")
    print(f"   √âpoques: {epochs}")
    print(f"   Batches/√©poque: {batches_per_epoch}")
    print(f"   Temps estim√©: {estimated_time/60:.1f} minutes")
    
    return estimated_time

def main():
    """Diagnostic et optimisation principale"""
    
    print("OPTIMISATION DES PERFORMANCES D'ENTRA√éNEMENT")
    print("=" * 60)
    
    # 1. Diagnostic syst√®me
    diagnose_system()
    
    # 2. Optimisation TensorFlow
    optimize_tensorflow()
    
    # 3. Test avec un mod√®le simple
    try:
        from models import create_model
        
        print(f"\nüß™ Test avec U-Net Mini...")
        model = create_model('unet_mini', (224, 224, 3), 8)
        
        # Benchmark
        results = benchmark_model_speed(model)
        
        # Configuration optimis√©e
        callbacks, optimizer = create_fast_training_setup()
        
        # Estimation pour votre cas
        print(f"\nüìä ANALYSE DE VOTRE CAS:")
        print(f"   Mod√®le: ~2M param√®tres")
        print(f"   Temps actuel: 40 min/√©poque")
        
        # Si 40min pour 1 √©poque, calculer le throughput actuel
        # Supposons ~2000 √©chantillons par √©poque (estimation)
        current_samples_per_min = 2000 / 40
        print(f"   Throughput actuel: ~{current_samples_per_min:.1f} √©chantillons/minute")
        
        if len(tf.config.list_physical_devices('GPU')) == 0:
            print(f"\n‚ö†Ô∏è  PROBL√àME PRINCIPAL: PAS DE GPU")
            print(f"   üöÄ Avec GPU: ~2-5 minutes/√©poque attendues")
            print(f"   üêå Avec CPU: 30-60 minutes/√©poque (normal)")
    
    except ImportError:
        print("‚ö†Ô∏è  Module models non trouv√©")
    
    # Recommandations
    print(f"\n" + "="*60)
    print("RECOMMANDATIONS PRIORITAIRES")
    print("="*60)
    
    if len(tf.config.list_physical_devices('GPU')) == 0:
        print("üéØ 1. UTILISER UN GPU (priorit√© absolue)")
        print("   - Google Colab avec GPU T4/V100")
        print("   - Kaggle Notebooks avec GPU")
        print("   - AWS/GCP avec instances GPU")
        
    print("üéØ 2. OPTIMISER LE BATCH SIZE")
    print("   - Tester batch_size=16, 32, 64")
    print("   - Plus grand = plus rapide (si m√©moire suffisante)")
    
    print("üéØ 3. R√âDUIRE LA TAILLE DES DONN√âES")
    print("   - Images 224x224 au lieu de 512x512")
    print("   - Sous-√©chantillonnage du dataset")
    
    print("üéØ 4. UTILISER MIXED PRECISION")
    print("   - tf.keras.mixed_precision.set_global_policy('mixed_float16')")
    
    print("üéØ 5. OPTIMISER LE PIPELINE DE DONN√âES")
    print("   - .cache(), .prefetch(), num_parallel_calls=AUTOTUNE")

if __name__ == "__main__":
    main()